{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Using Neural Netowrks  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Network Image](neural-network.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hello! \n",
    "\n",
    "If you're here, you're probably a member of the Hamilton lab and you probably want or need to figure out how to use neural networks for your projects and other stuff. Great! This notebook should provide you with everything you need to get started. \n",
    "\n",
    "While this will be far from a comprehensive manual on machine learning, it'll definitely provide you with a full working example of how to code up an input pipeline, define and run a model, and what's required to do so. It will also introduce you to the concepts and operations underlying neural networks and some resources to learn more about them. \n",
    "\n",
    "This notebook will specifically work on basic feed forward netowrks, sometimes called multi-layer perceptrons (MLPs) or artificial neural networks (ANNs). We're going to use them to classify stuff, which is generally called supervised learning. Supervised learning is certainly not the only kind of task you can perform with ANNs, it just forms a good basis for lab project applications like decoding, encoding, and EEG analysis.\n",
    "\n",
    "There are some other types of networks like convolutional and recurent networks, which perform different operations. I'll cover those in a different notebook, as they are a tad different in pretty substantial ways.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "## Getting Started \n",
    "### Requirements and Instalation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything we'll be doing requires python 3.6 and requires some extra modules you may not have.  \n",
    "To work with these examples, you're going to need some of the following modules on your machine:\n",
    "\n",
    "* __Numpy__ - This is python's linear algebra package.\n",
    "* __Tensorflow__ (`tf`) - Google's machine learning library (Extensive and powerful, but takes some time to master).\n",
    "* __Keras__ - High level neural net library that uses Tensorflow backend (Almost as flexible as `tf`, but far easier to get up and running).\n",
    "\n",
    "If you don't have these, they can be installed several ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'd recommend getting Anaconda and creating a conda environment with these packages. Anaconda is a package manager useful for scientific computing.  \n",
    "_[More on getting Anaconda](https://www.anaconda.com/distribution/)._\n",
    "\n",
    "**Note:** It is necessary to install both Keras and Tensorflow for this notebook, but you'll use only one. Using the tensorflow module `tf.keras`  is almost identical to standalone `keras`, though the `tf.keras` uses slightly different stuff under the hood. You can use either one to run the examples in this notebook, but I'd reccomend `keras` for now.  Once you get the hang of keras, you can move on to or even combine it with regular tensorflow if you want to do something really fancy! \n",
    "\n",
    "##### After you install anaconda, you'll run something like this in your terminal:  \n",
    "`conda create -n tensorflow_env tensorflow keras`   _* mind the spaces!_   \n",
    "   \n",
    "Here, `-n` is the name creation flag, `tensorflow_env` is the name of the environment passed to `-n`, and `tensorflow` and `keras` are the primary packages being installed. Anaconda will grab everything else you need to make these work!  \n",
    "You can use any name you want for your environment by swapping `tensorflow_env` with `your_env_name`.\n",
    "\n",
    "_* For more on installing tensorflow with anaconda, follow the instructions [here](https://www.anaconda.com/tensorflow-in-anaconda/)._ \n",
    "\n",
    "__Note:__ You may have to up- or down-grade the `tf` version. As of this writing, the stable versions are 1.12 or 1.13, but 2.0 has been rolled out. This isn't too big of an issue as we'll be using Keras anyway, and we only need a compatable version of tensorflow to work with Keras. To do so, replace `tensorflow` with `tesnorflow==1.12`  \n",
    "\n",
    "##### To activate your environment, run:  \n",
    "`conda activate tensorflow_env`\n",
    "  \n",
    "When active, your terminal will look something like:  \n",
    "    \n",
    "`(my_env_name) userid:/cur/open/path$` \n",
    "    \n",
    "##### Then set the correct python version:  \n",
    "\n",
    "`conda install python==3.6`  \n",
    "\n",
    "_* You can use pip instead of conda here._\n",
    "    \n",
    "_* Make sure pip is up to date!!!_\n",
    "\n",
    "For everyting you'll ever need on Keras, check out [this](https://keras.io/) resource.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** \n",
    "\n",
    "### Check to see if the modules work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras as K \n",
    "\n",
    "# This will error if something above didn't work!\n",
    "# Just activate your env and pip install your stuff again in that env to fix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "##  On to Networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First things First \n",
    "\n",
    "Ok, so we've gotten our stuff set up. Now what?  \n",
    "\n",
    "Well, we need to know what a neural network is and how to use it. This isn't going to be a thourough description of all the math and moving parts behind a network, just a high level take on whats happening with some background and useable code examples that can be modified and applied to your projects. So lets start with what a network is, and how to use it!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Network and Layers \n",
    "Feed forward artificial neural networks (ANNs) stem from the way real neurons pass on information:  \n",
    "- receive input from the previous neuron(s).  \n",
    "- determine if input exceeds firing threshold via some activation function.\n",
    "- pass on supra-threshold information to the next neuron(s). \n",
    "- weights between neurons are determined via Hebbian learning.\n",
    "    - neurons with correlated firing strengthen their connections\n",
    "    - possible way of preserving/encoding memory in neural circuits. \n",
    "\n",
    "In real neurons, information is electrical current. The activation function is the difference between the summation of postsynaptic potentials and the neuron's threshold potential _(see [Kandel's book](https://neurology.mhmedical.com/content.aspx?bookid=1049&sectionid=59138629) for more)_. The physiological implementation of Hebbian learning can be seen as long and short term potentiation or depression in a neural circuit (_[more](https://neurology.mhmedical.com/content.aspx?bookid=1049&sectionid=59138710) from Kandel_), altering the dynamics of how neurons interact with eachother (ie, firing rates, synaptic strength).\n",
    "\n",
    "In artificial neural networks, information is encoded as numerical values and we use mathmatical functions to determine activation. ANNs can be described as a series of layers that feed forward from an input layer, through an arbitrary number of hidden layers, to a final output layer wherein each successive layer recieves activated information from the previous layer as input.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What this actually means for us.  \n",
    "Each layer is actually a series of a few components: Input, weights, biases, and the activation.  \n",
    "\n",
    "At a high level, the layer weights are represented as a matrix which are multiplied with the input, representing how important each element at position $i,j$ in the weight matrix $W$ actually is for determining the correct output. Similarly, biases is represented as a vector, usually denoted $b$, that is added to after the input by weight multiplication to further shape how each part of the input affects the output.  Finally, after this multiplication and addition of the input, weights, and biases, the layer activation function is applied to give us an output.  \n",
    "\n",
    "In terms of linear algebra, we get something looking like this:\n",
    "$$ H_i = \\sigma(H_{i-1}W + b)$$\n",
    "\n",
    "   where $\\sigma$ represents our activation function, $H_i$ is our current layer, and $H_{i-1}$ is our previous layer (or input if this is the first layer). \n",
    "\n",
    "[Taken from Eli Bendersky's site (Most useful intro to matrix algebra ever.. maybe...).](https://www.google.com/url?sa=i&source=images&cd=&ved=2ahUKEwjhxMmt28HjAhUQXK0KHY-QC3YQjRx6BAgBEAU&url=https%3A%2F%2Feli.thegreenplace.net%2F2015%2Fvisualizing-matrix-multiplication-as-a-linear-combination%2F&psig=AOvVaw2qW7QJKhb4dKOw6lbPtf-v&ust=1563650763180555)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "### What this looks like in Keras:\n",
    "When we define a layer, we're not creating an individual neuron, but a layer of $k$ many neurons.  \n",
    "We do this like so: \n",
    "```\n",
    "from keras.layers import Dense \n",
    "\n",
    "layer1 = Dense(units=K_neurons, activation=chosen_activation)\n",
    "```\n",
    "`from keras.layers` becomes `from tensorflow.python.keras.layers` if using tensorflow instead of Keras directly.  \n",
    "_* This is the only change. Nice, right?_\n",
    "\n",
    "\n",
    "Seems legit, but what do we do for a network? What really is a Netowrk? \n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a Network for Classification\n",
    "\n",
    "<img src=\"dnn_fig.jpeg\" alt=\"color net\" style=\"width: 640px;\"/>\n",
    "\n",
    "### General Def: \n",
    "\n",
    "The ANN/MLP framework learns how to categorize which arbitrary features within an input sample are meaningful representations of a higher level class membership. It does so by passing on the activated output of one layer to the next, where the final layer computes a probability distribution based on the pattern of layer activations it receives. This distribution is then used to assign each input sample to the class it has the highest probability of belonging to. \n",
    "\n",
    "### Our take on things:\n",
    "If this sounds complicated, don't worry. In practice, a network is just a stack of these Keras layers we described earlier.   \n",
    "The layers take an input at one end and give you an output at the other. Everything else is handled by Keras! \n",
    "\n",
    "I'll get a bit more into the details of how to train a model to perform classification, but first, lets look at what coding a model looks like.  It really can be easy! \n",
    "\n",
    "_(There will be more detailed notebooks in the future, but you can check out_ [wiki](https://en.wikipedia.org/wiki/Artificial_neural_network), _[Kera's docs](https://keras.io/getting-started/sequential-model-guide/#examples), [Work by Geoffrey Hinton](https://scholar.google.com/citations?user=JicYPdAAAAAJ&hl=en&oi=sra) for deeper explanations of ANNs)_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Model Code\n",
    "\n",
    "The following cell is going to create a four layer feed forward network. I'll explain whats going on below the code, but just take a look at if for now.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "\n",
    "# This sets up a keras class for a feed forward (sequential) model.\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=32, activation='relu', input_shape=(Num_input_data_columns,)))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dense(units=Num_categories, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like that, your model exists, has been trained and has spat out some test metrics! \n",
    "***\n",
    "#### So what's in the code? \n",
    "Assigning `model = Sequential()` instantiates the Keras Sequential model class. This is a placeholder allowing you to arbitrarily define models. It has many usefull class methods and attributes that'll make building, running, and debugging models pretty easy.\n",
    "\n",
    "After this, we can use the `.add` method to add layers of our choosing to the model. Calls to `.add` stack layers in the sequential order of which `.add` is called (hence the class name `Sequential`).  \n",
    "\n",
    "`Dense` is the specific type of layer used. It is a `2d` layer, expecting to see your rows hold individual samples and your columns to hold meaningful info.  You may use other layers than `Dense` later, but they follow the same gist.\n",
    "\n",
    "Here's what each of these arguemnts passed to `Dense` do.\n",
    "\n",
    "* `units`: This is the number of neurons used in this layer.\n",
    "   - Its pretty typical (but not necessary) to start with a lower power of two, and jump to the next power with the next layer.  \n",
    "   - Alternatively, start with the number of (or a multiple of) feature channels in your input. \n",
    "   - Determining the number of units is largely experimental, so start somewhat small and test what works!\n",
    "\n",
    "\n",
    "* `activation`: this is the specific activation function used. More about these later on.\n",
    "\n",
    "\n",
    "* `input_shape`: defines the number of ___feature columns___ in your input data, where the input shape is * `(batch_size, features)`.\n",
    "\n",
    "    - `batch_size` is the number of samples passed in at a given step and `features` is the number of descriptive elements (time points, attributes, etc.) present in a sample. __*Read `input_shape` as (rows, columns)*__.\n",
    "    - `input_shape` is only needed in the first layer. Once it has this, Keras takes care of handling all the other resizing for you! \n",
    "\n",
    "Now that we have the code for the model, we just have to specify whats going when performing a classification task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## Supervised Learning, and other fancy names associated with Classification\n",
    "###### We'll be keeping a sky high level for the following concepts as you won't need to know all the nuts and bolts to use the code. However,  I've included links to more thorough explanations that I'd recommend going through  at some point.\n",
    "\n",
    "### What is Supervised Learning?\n",
    "\n",
    "___Supervised learning___ is the machine learning world's name for almost any kind of *classification task*.  This means that we are trying to get an ANN to distinguish different features or categories present within a dataset, while we already know which labels (also called ground truth) belong to each sample in the set. \n",
    "#### Uh.. OK... How do we do this? \n",
    "\n",
    "### Training-Lite\n",
    "The way a model \"learns\" is by passing in a bunch of data points at a time, correcting the model's guess as to what those data points were, and then repeating this process a bunch of times. \n",
    "\n",
    "This is handled by the `model.fit` call in the code block below, thanks to Keras. However to understand this call, we need to talk about about how this actually works.\n",
    "\n",
    "Training is an __iterative process__ in that the network guesses the labels for a batch of samples in discrete steps. At every step, guesses are made and scored. The 'supervised' component comes into play because we use a set of labels as an answer key to teach the model how to perform better, at each iteration.\n",
    "\n",
    "\n",
    "### Fancy Names \n",
    "\n",
    "So remember how our final model layer computes some probability density, and uses that to pick the most likely class for a sample? Well we need to have a way of correcting this process and teaching the model how to do a better job.\n",
    "\n",
    "#### Loss and the Objective Function\n",
    "That correction takes the form of whats called an ***objective function***, which is used while training the model. Keeping with our high level take, the objective funtion evaluates how well the model is performing by comparing its guesses to the corresponding labels. \n",
    "\n",
    "For most classification tasks, [categorical cross entropy](https://en.wikipedia.org/wiki/Cross_entropy) (also called Softmax or Softmax loss) is used as the ___objective function.___   \n",
    "Basically, it quantifies how much information from the model's guessed label distribution is needed to correctly predict the real labels, at each step. This comparison returns a ___loss value___ (low is good, high is bad).  ___Loss___ is the primary metric for model evaluation, and is the metric used in network performance ___optimization___.  \n",
    "\n",
    "   - ___Note___ _the objective function is also called the loss or cost function._  \n",
    "\n",
    "#### Optimization\n",
    "Loosely, optimization is how a network learns during the training proceedure. It is the process of correcting a network to maximize performance.  It does this by using an ___Optimization Algorithm___ that informs how changes to the weights and biases of model layers should be made, and how those changes affect performance. This is usually instantiated with some form of ___Stochastic Gradient Descent___ and ___Backpropagation___.  These are pretty thorny concepts, so I'll briefly talk about what is essential and include links to more detailed explanations.  \n",
    "##### Optimization with [Gradient Descent](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)\n",
    "\n",
    "There are [several](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.htmlA) specific algorithms that perform Gradient Descent with their own flavor, but they all esentially do the same thing: find the lowest loss of the objective function.  There are tons of ways to think about this, the most common is to think of the learning process of being a three dimensional gradient like a rough hill. The algorithm is trying to iteratively move in the opposite direction of the hill's highest point (the gradient) to find the lowest point, wherein the hill represents the loss in classification performance ([not my metaphor](https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html)).  \n",
    "\n",
    "##### [Backpropagation](https://skymind.ai/wiki/backpropagation) (Backprop for short)\n",
    "\n",
    "___Backpropagation___ is the process of _how_ the information from the gradient descent process affects the network. Because networks are trained iteratively, information from a sample is passed forward to make a guess at each step, but the resulting performance of that guess can be passed backwards as well in the same step. This backwards pass is used to alter the weights and biases of all layers in the network based on a given training step's error or ___loss value___. If the change made at the previous training step increases loss, the __optimization algorithm__ changes direction (literally) with respect to the gradient, and backprops that change through the netowrk.  If the change decreases loss, we keep moving down hill.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pieces of the .py\n",
    "\n",
    "In Keras, this is all set up for us in the `model.compile` call. \n",
    "\n",
    "- `loss` sets the __objective function__, and `optimizer` sets the specific __optimization algorithm__. \n",
    "\n",
    "`model.fit` handles all the training logic.\n",
    "\n",
    "`model.evaluate` handles all the testing.\n",
    "\n",
    "##### This is all there is to setting up your own neural networks with Keras! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pick your objective/loss function, optimizer, and what metrics to report\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# to train: \n",
    "# training data, training labels, how many times to go through data,\n",
    "# and how many datapoints are shown per training steps\n",
    "\n",
    "model.fit(train_data, train_labels, epochs=5, batchsize=32)\n",
    "\n",
    "# to evaluate:\n",
    "# test data, test labels, how many samples at a time (use as many as you can without crashing)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=128)\n",
    "print(loss_and_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## All Together Now\n",
    "\n",
    "We've jumped through a ton of ideas and concepts (if you did in fact read that absolute wall of text).\n",
    "Luckily with Keras, coding this is a breeze. \n",
    "\n",
    "Lets look at the code needed to actually do this stuff! \n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "\n",
    "### We'll need some data! \n",
    "\n",
    "Lets look at a mock-EEG analysis where we're trying to differentiate four conditions in an existing experiement.  \n",
    "\n",
    "Our data will be randomly generated, but the pipeline will be almost exactly what you'd use if you had your real data.\n",
    "\n",
    "Now just create and assign! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "\n",
    "# Dummy data\n",
    "\n",
    "x_train = np.random.random((1000,64))  # 1,000 samples of 64 EEG channels\n",
    "y_train = keras.utils.to_categorical(np.random.randint(4, size=(1000, 1)), num_classes=4) \n",
    "#4 conditions = 4 classes\n",
    "\n",
    "x_test = np.random.random((100,64))  # 100 samples of 64 EEG channels\n",
    "y_test = keras.utils.to_categorical(np.random.randint(4, size=(100, 1)), num_classes=4) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split the data into `x, y` pairs of `data, labels`.  \n",
    "\n",
    "We want different data for `train` and `test` sets, so we can evaluate how well the model performs on the problem, rather than how well it can recognize the corrected answers. (Though if it can't do that either, we have problems..)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization\n",
    "\n",
    "Normalization is the process of correcting inherent biases in your data.  Simply, it shifts all your data into the same scale so differences between points are somewhat proportional.  \n",
    "\n",
    "[Wiki](https://en.wikipedia.org/wiki/Feature_scaling), and [a good explanation](https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029)\n",
    "\n",
    "There are different ways of doing this that work best for specific problems. Here we're z-scoring the data (also called mean normalization), where for each value we subtract the mean and divide by the standard deviation.\n",
    "That way, each point reflects how different it is with respect to the distribution of all data points.  \n",
    "\n",
    "For our data, we want to do this per EEG channel (each of the 64 columns). With numpy, this is pretty easy.  \n",
    "\n",
    "- __*Note*__: when passing a value to the `axis` argument with `np` array functions, you're picking which axis you want a single value for. ***You are not selecting which axis to perform the function on!*** \n",
    "    - For things like `array.mean` and `array.std`, it usually means which axis you're getting rid of.\n",
    "    - `mean(axis=0)` you'll get the average value of all rows, for each index in the row (the number of columns)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Data\n",
    "\n",
    "x_train = (x_train - x_train.mean(axis=0)) / (x_train.std(axis=0))\n",
    "x_test = (x_test - x_test.mean(axis=0) / x_train.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Pipe it in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "\n",
    "\n",
    "Num_input_data_columns = 64\n",
    "Num_categories = num_conditions = 4 \n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(units=64, activation='relu', input_shape=(Num_input_data_columns,)))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=Num_categories, activation='softmax'))\n",
    "\n",
    "\n",
    "# Pick your objective/loss function, optimizer, and what metrics to report\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# to train: \n",
    "# training data, training labels, how many times to go through data,\n",
    "# and how many datapoints are shown per training steps\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=32)\n",
    "\n",
    "# to evaluate:\n",
    "# test data, test labels, how many samples at a time (use as many as you can without crashing)\n",
    "loss_and_metrics = model.evaluate(x_test, y_test, batch_size=32)\n",
    "print(loss_and_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we did about chance!\n",
    "\n",
    "\n",
    "Now, obviously we don't expect that run to have been spectacular as we're using randomly generated data.  \n",
    "\n",
    "Lets look at what happens when we use real data with a model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mnist\n",
    "\n",
    "Mnist stands for Modified National Institute of Standards and Technology.\n",
    "\n",
    "They've put together __*the*__ standard in machine learning and neural netowrk problems: handwritten digit classification. Here, we're trying to get a model that can tell apart handwritten digits 0 - 9.  To see what neural nets can do, lets take a look at the mnist problem!\n",
    "\n",
    "Keras has the Mnist dataset available as a module that we can load."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-36d138379001>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline \n",
    "\n",
    "num_classes = 10 # digits 0:9\n",
    "\n",
    "(x_train_mnist, y_train_mnist), (x_test_mnist, y_test_mnist) = mnist.load_data()\n",
    "print('training data shape: ', x_train_mnist.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at what we've got!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-f2c7a9aa9a3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtight_layout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train_mnist\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mix\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mplot_im\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train_mnist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m28\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig,ax = plt.subplots(2,5)\n",
    "fig.tight_layout()\n",
    "for ix, ax in enumerate(ax.flatten()):\n",
    "    im = np.argwhere(y_train_mnist == ix)[0]\n",
    "    plot_im = np.reshape(x_train_mnist[im], (28,28))\n",
    "    ax.imshow(plot_im, cmap='gray_r')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sweet! Now lets normalize our data.\n",
    "\n",
    "Because these are images, we're going to normalize them a little differently. Instead of z-scoring them, we can just divide by the max value for a given pixel which will is 255 for RGB images.  \n",
    "\n",
    "We'll also reshape the data from its current 3D to 2D so we can fit the proper number of samples and channels. We'll do this by keeping the number of samples, `x_train.shape[0]`, and collapsing the other dimensions. As we printed above, we have $6000$  $28 x 28$ images. We'll flatten each image by appending the two dimensions giving us $ 28 x 28 = 784$ columns.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "x_train_mnist = x_train_mnist.reshape(60000, 784) # 784 = 28 * 28\n",
    "x_test_mnist = x_test_mnist.reshape(10000, 784)\n",
    "x_train_mnist = x_train_mnist.astype('float32')\n",
    "x_test_mnist = x_test_mnist.astype('float32')\n",
    "x_train_mnist /= 255\n",
    "x_test_mnist /= 255\n",
    "print(x_train_mnist.shape[0], 'train samples')\n",
    "print(x_test_mnist.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we convert our labels to the standard 'one hot' format. \n",
    "\n",
    "This means we have a $(Samples \\ x\\ Classes)$ matrix, where the column corresponding to our label gets a one, and everything else gets a zero, per row.  \n",
    "\n",
    "It looks like this: \n",
    "\n",
    "    labels = [[0, 0, 0, 1],\n",
    "              [1, 0, 0, 0], \n",
    "              ...\n",
    "              [0, 1, 0, 0]] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_mnist = keras.utils.to_categorical(y_train_mnist, num_classes)\n",
    "y_test_mnist = keras.utils.to_categorical(y_test_mnist, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lets see what we get!\n",
    "\n",
    "Just as before, we'll make our model, fit it, then evaluate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.7406 - acc: 0.7943\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.2961 - acc: 0.9144\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.2365 - acc: 0.9312\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 2s 27us/step - loss: 0.1982 - acc: 0.9415\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1707 - acc: 0.9499\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1509 - acc: 0.9561\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1360 - acc: 0.9597\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 1s 24us/step - loss: 0.1234 - acc: 0.9637\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 1s 25us/step - loss: 0.1129 - acc: 0.9676\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1041 - acc: 0.9699\n",
      "10000/10000 [==============================] - 0s 14us/step\n",
      "final loss value = 0.11312637349292635\n",
      "final accuracy =  0.9665\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense \n",
    "\n",
    "model_mnist = Sequential()\n",
    "\n",
    "model_mnist.add(Dense(units=64, activation='relu', input_shape=(784,)))\n",
    "model_mnist.add(Dense(units=64, activation='relu'))\n",
    "model_mnist.add(Dense(units=64, activation='relu'))\n",
    "model_mnist.add(Dense(units=num_classes, activation='softmax'))\n",
    "\n",
    "\n",
    "# Pick your objective/loss function, optimizer, and what metrics to report\n",
    "model_mnist.compile(loss='categorical_crossentropy',\n",
    "              optimizer='sgd',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# to train: \n",
    "# training data, training labels, how many times to go through data,\n",
    "# and how many datapoints are shown per training steps\n",
    "\n",
    "model_mnist.fit(x_train_mnist, y_train_mnist, epochs=10, batch_size=32)\n",
    "\n",
    "# to evaluate:\n",
    "# test data, test labels, how many samples at a time (use as many as you can without crashing)\n",
    "loss_and_metrics = model_mnist.evaluate(x_test_mnist, y_test_mnist, batch_size=32)\n",
    "print('final loss value =', loss_and_metrics[0])\n",
    "print('final accuracy = ', loss_and_metrics[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It works!\n",
    "\n",
    "If you want to toy around with things, see what happens when you change the number of nodes per layer, the number of epochs, or the batch size.  These can have a huge impact on your results, and are often empirically decided, so have at it! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Thats all\n",
    "\n",
    "This is all you need to get started on your own neural network projects!\n",
    "\n",
    "In the future, I'll make a few notebooks where we tackle convolutional neural networks, heavy data processing and other more advanced concepts.\n",
    "\n",
    "Thanks for checking this out! \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
